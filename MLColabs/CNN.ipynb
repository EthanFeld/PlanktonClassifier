{"cells":[{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"TaBQE9Ncp_OW"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucg27_VGa2ze","outputId":"fa83def5-0a36-4910-aa63-f6346dffe3d1","executionInfo":{"status":"ok","timestamp":1755880608139,"user_tz":420,"elapsed":71452,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"vP58iGu1bQhB","executionInfo":{"status":"ok","timestamp":1755881315385,"user_tz":420,"elapsed":5,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[],"source":["# Import all packages\n","import cv2\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWGIz17JbBu8","outputId":"9d2ebba8-7928-4e37-9b59-37be5bb765d7","executionInfo":{"status":"ok","timestamp":1755880767626,"user_tz":420,"elapsed":10,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Starting to Scan Image Folders ---\n","--> Processing main folder: /content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2006\n","--> Processing main folder: /content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2007\n","--> Processing main folder: /content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2013\n","--> Processing main folder: /content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2014-VALIDATION YEAR\n","\n","...Initial file scan complete!\n"]}],"source":["folder_paths = ['/content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2006','/content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2007','/content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2013', '/content/drive/MyDrive/Data/Plankton/WHOI_unzipped_data/2014-VALIDATION YEAR']\n","\n","all_images_data = []\n","\n","print(\"--- Starting to Scan Image Folders ---\")\n","for folder_path in folder_paths:\n","    print(f\"--> Processing main folder: {folder_path}\")\n","\n","    for root, subdirs, files in os.walk(folder_path):\n","        if files:\n","            image_class = os.path.basename(root)\n","\n","            for file_name in files:\n","                file_path = os.path.join(root, file_name)\n","\n","                # Append the data to our list\n","                all_images_data.append({\n","                    'Image Class': image_class,\n","                    'Image Path': file_path\n","                })\n","\n","print(\"\\n...Initial file scan complete!\")\n","\n","# Create a Pandas DataFrame from the collected path data\n","path_df = pd.DataFrame(all_images_data)"]},{"cell_type":"markdown","metadata":{"id":"44HuCsnpFs_G"},"source":["## Load Data"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tWUdTT_IujmV","executionInfo":{"status":"ok","timestamp":1755880945603,"user_tz":420,"elapsed":6269,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[],"source":["load_path = '/content/drive/MyDrive/Plankton/image_dataframe.pkl'\n","with open(load_path, 'rb') as f:\n","    loaded_img_dataframe = pickle.load(f)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVVyVa9VFuFa","outputId":"5eac06a1-3114-4b3a-dd14-c5a67d29bc75","executionInfo":{"status":"ok","timestamp":1755880985924,"user_tz":420,"elapsed":149,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(      Image_Class                                       Image_Matrix\n"," 0             mix  [[204, 204, 204, 203, 203, 203, 203, 200, 202,...\n"," 1             mix  [[200, 201, 203, 201, 199, 200, 202, 201, 199,...\n"," 2             mix  [[221, 222, 223, 224, 223, 222, 221, 222, 222,...\n"," 3             mix  [[192, 191, 188, 183, 182, 189, 185, 180, 185,...\n"," 4             mix  [[212, 212, 212, 212, 213, 213, 213, 211, 212,...\n"," ...           ...                                                ...\n"," 10995      dino30  [[194, 193, 194, 195, 195, 195, 192, 193, 194,...\n"," 10996      dino30  [[204, 205, 207, 209, 209, 208, 207, 207, 206,...\n"," 10997      dino30  [[179, 189, 187, 183, 191, 186, 186, 187, 194,...\n"," 10998      dino30  [[215, 216, 217, 219, 218, 218, 217, 217, 217,...\n"," 10999      dino30  [[205, 205, 205, 204, 204, 206, 206, 205, 205,...\n"," \n"," [11000 rows x 2 columns],\n"," (11000, 2),\n"," (45, 113))"]},"metadata":{},"execution_count":10}],"source":["loaded_img_dataframe, loaded_img_dataframe.shape, loaded_img_dataframe['Image_Matrix'][0].shape"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKnJ4nE4IrSF","outputId":"08aadf2f-f18f-4d47-80d6-36e347d0e768","executionInfo":{"status":"ok","timestamp":1755881138526,"user_tz":420,"elapsed":941,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Image resizing complete.\n","\n","Number of images that failed to resize: 239\n","DataFrame shape after removing failed images: (10761, 3)\n","Label encoding complete.\n","\n","Cleaned DataFrame head with new columns:\n","  Image_Class  Image_Class_Encoded  \\\n","0         mix                   20   \n","1         mix                   20   \n","2         mix                   20   \n","3         mix                   20   \n","4         mix                   20   \n","\n","                                  Image_Matrix_64x64  \n","0  [[204, 204, 203, 203, 201, 204, 204, 202, 204,...  \n","1  [[200, 203, 199, 201, 200, 199, 200, 199, 200,...  \n","2  [[221, 223, 223, 222, 222, 222, 222, 221, 220,...  \n","3  [[192, 189, 182, 189, 181, 187, 189, 185, 188,...  \n","4  [[212, 212, 213, 213, 212, 213, 214, 213, 213,...  \n"]}],"source":["TARGET_SIZE = (64, 64)\n","def resize_image_robust(image_matrix):\n","    \"\"\"\n","    Safely resizes an image matrix.\n","    Returns the resized image or None if an error occurs.\n","    \"\"\"\n","    try:\n","        # Convert to a numpy array, just in case it's a list\n","        img_array = np.array(image_matrix)\n","        # Check for empty or invalid array after conversion\n","        if img_array.size == 0:\n","            return None\n","        # Ensure the data type is uint8, which is required by cv2.resize\n","        if img_array.dtype != np.uint8:\n","            img_array = img_array.astype(np.uint8)\n","        # Perform the resize\n","        return cv2.resize(img_array, TARGET_SIZE, interpolation=cv2.INTER_AREA)\n","    except (TypeError, ValueError) as e:\n","        return None\n","# Apply the ROBUST resizing function\n","loaded_img_dataframe['Image_Matrix_64x64'] = loaded_img_dataframe['Image_Matrix'].apply(resize_image_robust)\n","print(\"Image resizing complete.\")\n","# Post-processing: Handle failed resizes\n","failed_count = loaded_img_dataframe['Image_Matrix_64x64'].isnull().sum()\n","print(f\"\\nNumber of images that failed to resize: {failed_count}\")\n","# Remove the rows where resizing failed before proceeding\n","cleaned_dataframe = loaded_img_dataframe.dropna(subset=['Image_Matrix_64x64']).copy()\n","print(f\"DataFrame shape after removing failed images: {cleaned_dataframe.shape}\")\n","\n","# Encode the 'Image_Class' column on the cleaned data\n","label_encoder = LabelEncoder()\n","cleaned_dataframe['Image_Class_Encoded'] = label_encoder.fit_transform(cleaned_dataframe['Image_Class'])\n","print(\"Label encoding complete.\")\n","\n","# Display the updated DataFrame\n","print(\"\\nCleaned DataFrame head with new columns:\")\n","print(cleaned_dataframe[['Image_Class', 'Image_Class_Encoded', 'Image_Matrix_64x64']].head())"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"An1wzKRJH6qw","outputId":"10a12565-b47b-4d16-d061-103ddfc95f31","executionInfo":{"status":"ok","timestamp":1755881143390,"user_tz":420,"elapsed":19,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Index(['Image_Class', 'Image_Matrix', 'Image_Matrix_64x64',\n","        'Image_Class_Encoded'],\n","       dtype='object'),\n"," array([[209, 208, 209, ..., 203, 203, 203],\n","        [209, 208, 210, ..., 204, 204, 202],\n","        [209, 208, 209, ..., 204, 204, 203],\n","        ...,\n","        [207, 208, 208, ..., 203, 203, 202],\n","        [207, 206, 207, ..., 203, 204, 203],\n","        [208, 208, 209, ..., 204, 204, 203]], dtype=uint8),\n"," np.int64(7))"]},"metadata":{},"execution_count":14}],"source":["cleaned_dataframe.columns, cleaned_dataframe['Image_Matrix_64x64'][1000], cleaned_dataframe['Image_Class_Encoded'][1000]"]},{"cell_type":"markdown","source":["## CNN Model"],"metadata":{"id":"Q9makxR3LFb-"}},{"cell_type":"code","source":["# Convert data to tensors\n","X = torch.tensor(np.array(cleaned_dataframe['Image_Matrix_64x64'].tolist()), dtype=torch.float32)\n","y = torch.tensor(cleaned_dataframe['Image_Class_Encoded'].values, dtype=torch.long)\n","# Reshape X for CNN input: [batch_size, channels, height, width]\n","X = X.view(-1, 1, 64, 64) # Gray scale: channels = 1\n","print(f\"X shape: {X.shape}\")\n","print(f\"y shape: {y.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGsVwZQWRVZq","executionInfo":{"status":"ok","timestamp":1755881269138,"user_tz":420,"elapsed":194,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}},"outputId":"fc0b2c7e-1d6d-457c-f5cd-6d8b3c9fcbd9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["X shape: torch.Size([10761, 1, 64, 64])\n","y shape: torch.Size([10761])\n"]}]},{"cell_type":"code","source":["# Split the model into train and test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Testing data shape:  {X_test.shape}\")\n","# Create separate dataloaders\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZ16M2pjqRFr","executionInfo":{"status":"ok","timestamp":1755881276112,"user_tz":420,"elapsed":150,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}},"outputId":"cabccf11-f56f-4d8d-f484-9d16442df8f0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape: torch.Size([8608, 1, 64, 64])\n","Testing data shape:  torch.Size([2153, 1, 64, 64])\n"]}]},{"cell_type":"code","source":["# Define the CNN model\n","class CNN(nn.Module):\n","  \"\"\"\n","       Parameters:\n","           * in_channels: Number of channels in the input image (for grayscale images, 1)\n","           * num_classes: Number of classes to predict.\n","       \"\"\"\n","  def __init__(self, in_channels, num_classes):\n","      super(CNN, self).__init__()\n","# Convolutional layers\n","      self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n","      self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","      self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","      self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","      self.dropout = nn.Dropout(0.3)\n","      self.relu = nn.ReLU()\n","\n","# Fully connected layers\n","      self.fc1 = nn.Linear(128 * 8 * 8, 512)\n","      self.fc2 = nn.Linear(512, num_classes)\n","  def forward(self, x):\n","    \"\"\"\n","      Parameters:\n","      x: Input Tensor\n","    \"\"\"\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = self.pool(F.relu(self.conv2(x)))\n","    x = self.pool(F.relu(self.conv3(x)))\n","    x = x.reshape(x.shape[0], -1)  # Flatten the tensor\n","    x = self.relu(self.fc1(x))\n","    x = self.dropout(x)\n","    x = self.fc2(x)\n","    return x\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","dataset = TensorDataset(X, y)\n","dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n","model = CNN(in_channels=1, num_classes=22).to(device)\n","\n","# Loss function for multi-class classification\n","criterion = nn.CrossEntropyLoss()\n","\n","# Adam optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","# Training loops\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_X, batch_y in train_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","\n","        # Backward pass and optimize\n","        loss.backward() # Comput gradients\n","        optimizer.step() # update paramers\n","\n","        # Track accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += batch_y.size(0)\n","        correct += (predicted == batch_y).sum().item()\n","        total_loss += loss.item()\n","\n","    # Print statistics\n","    avg_loss = total_loss / len(train_loader)\n","    accuracy = 100 * correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n","\n","# Evaluate on the test dataset\n","model.eval()\n","test_correct = 0\n","test_total = 0\n","with torch.no_grad():\n","    for batch_X, batch_y in test_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        outputs = model(batch_X)\n","        _, predicted = torch.max(outputs.data, 1)\n","        test_total += batch_y.size(0)\n","        test_correct += (predicted == batch_y).sum().item()\n","test_accuracy = 100 * test_correct / test_total\n","print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n","      f\"Train Loss: {avg_loss:.4f} | \"\n","      f\"Train Acc: {accuracy:.2f}% | \"\n","      f\"Test Acc: {test_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WneV2LHIXfLr","executionInfo":{"status":"ok","timestamp":1755813021045,"user_tz":420,"elapsed":119694,"user":{"displayName":"Lorraine Yang","userId":"07545198609006176311"}},"outputId":"0e2c3074-d925-4aa8-9407-4e286ebf98bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss: 3.3032, Accuracy: 22.12%\n","Epoch [2/100], Loss: 1.6399, Accuracy: 46.79%\n","Epoch [3/100], Loss: 1.3280, Accuracy: 57.65%\n","Epoch [4/100], Loss: 1.1047, Accuracy: 64.50%\n","Epoch [5/100], Loss: 0.9572, Accuracy: 69.19%\n","Epoch [6/100], Loss: 0.8654, Accuracy: 71.93%\n","Epoch [7/100], Loss: 0.7771, Accuracy: 74.41%\n","Epoch [8/100], Loss: 0.6864, Accuracy: 77.24%\n","Epoch [9/100], Loss: 0.6105, Accuracy: 79.01%\n","Epoch [10/100], Loss: 0.5443, Accuracy: 81.70%\n","Epoch [11/100], Loss: 0.4890, Accuracy: 83.84%\n","Epoch [12/100], Loss: 0.4241, Accuracy: 86.09%\n","Epoch [13/100], Loss: 0.3908, Accuracy: 86.87%\n","Epoch [14/100], Loss: 0.3303, Accuracy: 88.63%\n","Epoch [15/100], Loss: 0.2846, Accuracy: 90.64%\n","Epoch [16/100], Loss: 0.2666, Accuracy: 90.96%\n","Epoch [17/100], Loss: 0.2271, Accuracy: 92.48%\n","Epoch [18/100], Loss: 0.2075, Accuracy: 93.06%\n","Epoch [19/100], Loss: 0.1751, Accuracy: 93.99%\n","Epoch [20/100], Loss: 0.1739, Accuracy: 94.15%\n","Epoch [21/100], Loss: 0.1481, Accuracy: 94.93%\n","Epoch [22/100], Loss: 0.1409, Accuracy: 95.35%\n","Epoch [23/100], Loss: 0.1177, Accuracy: 96.11%\n","Epoch [24/100], Loss: 0.1093, Accuracy: 96.36%\n","Epoch [25/100], Loss: 0.1198, Accuracy: 96.18%\n","Epoch [26/100], Loss: 0.0884, Accuracy: 96.95%\n","Epoch [27/100], Loss: 0.0953, Accuracy: 96.82%\n","Epoch [28/100], Loss: 0.0869, Accuracy: 97.07%\n","Epoch [29/100], Loss: 0.0722, Accuracy: 97.50%\n","Epoch [30/100], Loss: 0.0633, Accuracy: 97.98%\n","Epoch [31/100], Loss: 0.0723, Accuracy: 97.68%\n","Epoch [32/100], Loss: 0.0634, Accuracy: 97.64%\n","Epoch [33/100], Loss: 0.0850, Accuracy: 97.07%\n","Epoch [34/100], Loss: 0.0736, Accuracy: 97.64%\n","Epoch [35/100], Loss: 0.0656, Accuracy: 97.91%\n","Epoch [36/100], Loss: 0.0647, Accuracy: 97.77%\n","Epoch [37/100], Loss: 0.0638, Accuracy: 97.76%\n","Epoch [38/100], Loss: 0.0494, Accuracy: 98.40%\n","Epoch [39/100], Loss: 0.0573, Accuracy: 98.23%\n","Epoch [40/100], Loss: 0.0622, Accuracy: 97.76%\n","Epoch [41/100], Loss: 0.0650, Accuracy: 97.71%\n","Epoch [42/100], Loss: 0.0508, Accuracy: 98.21%\n","Epoch [43/100], Loss: 0.0494, Accuracy: 98.46%\n","Epoch [44/100], Loss: 0.0560, Accuracy: 98.17%\n","Epoch [45/100], Loss: 0.0369, Accuracy: 98.83%\n","Epoch [46/100], Loss: 0.0371, Accuracy: 98.78%\n","Epoch [47/100], Loss: 0.0410, Accuracy: 98.54%\n","Epoch [48/100], Loss: 0.0275, Accuracy: 99.18%\n","Epoch [49/100], Loss: 0.0385, Accuracy: 98.83%\n","Epoch [50/100], Loss: 0.0391, Accuracy: 98.69%\n","Epoch [51/100], Loss: 0.0419, Accuracy: 98.71%\n","Epoch [52/100], Loss: 0.0554, Accuracy: 98.28%\n","Epoch [53/100], Loss: 0.0622, Accuracy: 97.93%\n","Epoch [54/100], Loss: 0.0368, Accuracy: 98.83%\n","Epoch [55/100], Loss: 0.0378, Accuracy: 98.85%\n","Epoch [56/100], Loss: 0.0501, Accuracy: 98.41%\n","Epoch [57/100], Loss: 0.0377, Accuracy: 98.74%\n","Epoch [58/100], Loss: 0.0282, Accuracy: 99.09%\n","Epoch [59/100], Loss: 0.0310, Accuracy: 98.95%\n","Epoch [60/100], Loss: 0.0376, Accuracy: 98.66%\n","Epoch [61/100], Loss: 0.0313, Accuracy: 98.99%\n","Epoch [62/100], Loss: 0.0426, Accuracy: 98.53%\n","Epoch [63/100], Loss: 0.0348, Accuracy: 98.78%\n","Epoch [64/100], Loss: 0.0375, Accuracy: 98.77%\n","Epoch [65/100], Loss: 0.0400, Accuracy: 98.74%\n","Epoch [66/100], Loss: 0.0237, Accuracy: 99.19%\n","Epoch [67/100], Loss: 0.0241, Accuracy: 99.18%\n","Epoch [68/100], Loss: 0.0288, Accuracy: 99.00%\n","Epoch [69/100], Loss: 0.0380, Accuracy: 98.74%\n","Epoch [70/100], Loss: 0.0449, Accuracy: 98.45%\n","Epoch [71/100], Loss: 0.0310, Accuracy: 99.00%\n","Epoch [72/100], Loss: 0.0360, Accuracy: 98.87%\n","Epoch [73/100], Loss: 0.0302, Accuracy: 98.95%\n","Epoch [74/100], Loss: 0.0317, Accuracy: 98.89%\n","Epoch [75/100], Loss: 0.0373, Accuracy: 98.76%\n","Epoch [76/100], Loss: 0.0273, Accuracy: 99.12%\n","Epoch [77/100], Loss: 0.0355, Accuracy: 98.94%\n","Epoch [78/100], Loss: 0.0300, Accuracy: 98.85%\n","Epoch [79/100], Loss: 0.0262, Accuracy: 99.07%\n","Epoch [80/100], Loss: 0.0365, Accuracy: 98.94%\n","Epoch [81/100], Loss: 0.0397, Accuracy: 98.83%\n","Epoch [82/100], Loss: 0.0196, Accuracy: 99.19%\n","Epoch [83/100], Loss: 0.0193, Accuracy: 99.33%\n","Epoch [84/100], Loss: 0.0354, Accuracy: 98.90%\n","Epoch [85/100], Loss: 0.0407, Accuracy: 98.72%\n","Epoch [86/100], Loss: 0.0240, Accuracy: 99.24%\n","Epoch [87/100], Loss: 0.0260, Accuracy: 99.15%\n","Epoch [88/100], Loss: 0.0298, Accuracy: 98.87%\n","Epoch [89/100], Loss: 0.0340, Accuracy: 98.90%\n","Epoch [90/100], Loss: 0.0214, Accuracy: 99.35%\n","Epoch [91/100], Loss: 0.0262, Accuracy: 99.05%\n","Epoch [92/100], Loss: 0.0207, Accuracy: 99.18%\n","Epoch [93/100], Loss: 0.0370, Accuracy: 98.87%\n","Epoch [94/100], Loss: 0.0315, Accuracy: 98.94%\n","Epoch [95/100], Loss: 0.0313, Accuracy: 98.99%\n","Epoch [96/100], Loss: 0.0264, Accuracy: 99.31%\n","Epoch [97/100], Loss: 0.0239, Accuracy: 99.17%\n","Epoch [98/100], Loss: 0.0239, Accuracy: 99.28%\n","Epoch [99/100], Loss: 0.0204, Accuracy: 99.27%\n","Epoch [100/100], Loss: 0.0337, Accuracy: 98.89%\n","Epoch [100/100] | Train Loss: 0.0337 | Train Acc: 98.89% | Test Acc: 77.62%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"16Yqgs0bXDW0VeJzp3xR4sSGYuEkFI7O1","timestamp":1755625617910}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}