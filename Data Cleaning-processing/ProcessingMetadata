{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from pathlib import Path\n","drive.mount('/content/drive')\n","\n","#!pip -q install pandas pyarrow fastparquet scikit-learn\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suAM4dgjHeNi","executionInfo":{"status":"ok","timestamp":1755812355140,"user_tz":420,"elapsed":3362,"user":{"displayName":"Ethan Feldman","userId":"01965624025368250252"}},"outputId":"767405ce-bb8a-4576-9030-4e25aae958e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\n","\n","# Point to the CSV produced earlier (with image paths + interpolated env vars)\n","MERGED_CSV = Path(\"/content/drive/MyDrive/Plankton/ifcb_merged_dataframe.csv\")\n","\n","# Optional output paths\n","OUT_PARQUET = Path(\"/content/drive/MyDrive/ifcb_cv_dataset2014.parquet\")\n","\n"],"metadata":{"id":"HAUDOPfwHooW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# -------- SETTINGS --------\n","# Resize all images to a consistent size for your model input\n","TARGET_SIZE = (64, 64)  # (width, height)\n","KEEP_GRAYSCALE_AS_RGB = True  # convert 1-channel to 3-channel RGB for consistency\n","STORE_IMAGE_BYTES = True      # set False if you only want metadata (lighter)\n","CHECK_FILES_EXIST = True\n","\n","# Load the merged table\n","df = pd.read_csv(MERGED_CSV)\n","# Minimal required columns assumed from the previous pipeline:\n","# 'image_path', 'label', 'bin_id', 'time', ... (plus your env variables)\n","\n","# Clean up paths / drop missing\n","df = df.dropna(subset=[\"image_path\"]).copy()\n","df[\"label\"] = df[\"image_path\"].apply(lambda x: Path(x).parent.name)\n","df = df[~df[\"label\"].str.contains(\"detritus|mix\", case=False, na=False)]\n","df[\"image_path\"] = df[\"image_path\"].astype(str)\n","df = df[df.groupby(\"label\")[\"label\"].transform(\"count\") >= 50]\n","\n","\n","if CHECK_FILES_EXIST:\n","    exists_mask = df[\"image_path\"].apply(lambda p: Path(p).is_file())\n","    missing = (~exists_mask).sum()\n","    if missing:\n","        print(f\"Warning: {missing} files not found; dropping missing.\")\n","    df = df.loc[exists_mask].reset_index(drop=True)\n","\n","# Helper: read + preprocess one image\n","def load_preprocess(path, target_size=TARGET_SIZE, keep_gray_as_rgb=KEEP_GRAYSCALE_AS_RGB):\n","    # Read as BGR (cv2 default). Use imdecode for robustness.\n","    # If you have some non-ASCII paths, imdecode handles better:\n","    data = None\n","    try:\n","        with open(path, \"rb\") as f:\n","            data = np.frombuffer(f.read(), dtype=np.uint8)\n","        img_bgr = cv2.imdecode(data, cv2.IMREAD_COLOR)  # force 3-channel if possible\n","    except Exception:\n","        img_bgr = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","\n","    if img_bgr is None:\n","        return None, None, None, None  # unreadable\n","\n","    # Determine original metadata\n","    if img_bgr.ndim == 2:\n","        h, w = img_bgr.shape\n","        c = 1\n","        img = img_bgr\n","    else:\n","        h, w, c = img_bgr.shape\n","        img = img_bgr\n","\n","    # Normalize to RGB\n","    if c == 1 and keep_gray_as_rgb:\n","        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","        c = 3\n","    elif c == 3:\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    elif c == 4:\n","        # Convert BGRA -> RGB (drop alpha)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n","        c = 3\n","    else:\n","        # Unknown channel layout; try to coerce to 3-channel\n","        if img.ndim == 2:\n","            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","            c = 3\n","        else:\n","            # fallback: take first 3 channels\n","            img = img[..., :3]\n","            c = 3\n","\n","    # Resize to target size (width, height)\n","    tw, th = target_size\n","    if (w, h) != (tw, th):\n","        img_resized = cv2.resize(img, (tw, th), interpolation=cv2.INTER_AREA)\n","    else:\n","        img_resized = img\n","\n","    # Encode to PNG bytes (RGB stays RGB in memory; PNG is lossless)\n","    img_bytes = None\n","    try:\n","        # cv2.imencode expects BGR; convert back temporarily for encoding\n","        bgr_for_encode = cv2.cvtColor(img_resized, cv2.COLOR_RGB2BGR)\n","        ok, buf = cv2.imencode(\".png\", bgr_for_encode)\n","        if ok:\n","            img_bytes = buf.tobytes()\n","    except Exception:\n","        img_bytes = None\n","\n","    return (h, w, c), img_resized.shape[:2], img_bytes, img_resized\n","\n","# Process images (iteratively to limit memory)\n","meta_rows = []\n","image_bytes_list = []  # optional\n","\n","for idx, row in df.iterrows():\n","    p = row[\"image_path\"]\n","    meta, resized_hw, png_bytes, _ = load_preprocess(p)\n","\n","    if meta is None:\n","        # unreadable image; skip row\n","        continue\n","\n","    (orig_h, orig_w, orig_c) = meta\n","    (r_h, r_w) = resized_hw\n","\n","    meta_rows.append({\n","        \"index_src\": idx,\n","        \"orig_height\": orig_h,\n","        \"orig_width\": orig_w,\n","        \"orig_channels\": orig_c,\n","        \"resized_height\": r_h,\n","        \"resized_width\": r_w,\n","    })\n","\n","    if STORE_IMAGE_BYTES:\n","        image_bytes_list.append(png_bytes)\n","\n","# Build metadata DataFrame and merge back\n","meta_df = pd.DataFrame(meta_rows)\n","cv_df = df.iloc[[r[\"index_src\"] for r in meta_rows]].reset_index(drop=True)\n","\n","cv_df = pd.concat([cv_df, meta_df.drop(columns=[\"index_src\"]).reset_index(drop=True)], axis=1)\n","\n","if STORE_IMAGE_BYTES:\n","    cv_df[\"image_png\"] = image_bytes_list  # bytes column (binary) suitable for Parquet/Feather\n","\n","# Optional: make train/val split (stratified by label if available)\n","if \"label\" in cv_df.columns and cv_df[\"label\"].notna().any():\n","    train_idx, val_idx = train_test_split(\n","        np.arange(len(cv_df)),\n","        test_size=0.2,\n","        random_state=42,\n","        stratify=cv_df[\"label\"] if cv_df[\"label\"].nunique() > 1 else None\n","    )\n","    cv_df.loc[:, \"split\"] = \"train\"\n","    cv_df.loc[val_idx, \"split\"] = \"val\"\n","else:\n","    cv_df[\"split\"] = \"train\"\n","\n","print(f\"Final CV-ready rows: {len(cv_df):,}\")\n","cv_df.head(3)\n"],"metadata":{"id":"v7OvXt8ZHwrx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parquet keeps the binary column efficiently\n","if STORE_IMAGE_BYTES:\n","    cv_df.to_parquet(OUT_PARQUET, index=False)\n","    print(\"Saved Parquet:\", OUT_PARQUET)\n","\n"],"metadata":{"id":"K-WufRY_Hx4Z"},"execution_count":null,"outputs":[]}]}